{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 10 GB of X, y\n",
    "\n",
    "y = w1*x + w0\n",
    "\n",
    "How big is the model? - 2 numbers\n",
    "\n",
    "The size of the model ->\n",
    "\n",
    "weights = (l1 + 1) * L2 + (l2+1)*l3 + (l3+1)*l4\n",
    "size of model = weights * size of weight -> weights * 16 bits/8 bytes\n",
    "\n",
    "Quantization can reduce the size because each weight will take fewer bits.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How do we intrepret the ANN\n",
    "\n",
    "1. It is bunch of knobs that gradient descent tweaks untill it starts to give the expected outcome - minimum loss(ypred, yactual).\n",
    "\n",
    "knobs != neuron\n",
    "knobs == connection between neurons. \n",
    "\n",
    "2. It is a chip that has boolean / search (attention) operations (AND, NOT, etc) figured by gradient descent\n",
    "\n",
    "3. Many models that turn input into another set of features. \n",
    "\n",
    "Input -> Layer1 -> New Features -> layer2 -> New Feature \n",
    "\n",
    "I -> l1 -> l2 -> l3 -> ....... --> y\n",
    "\n",
    "You can imagine these models to be decision trees or logistic regression etc.\n",
    "\n",
    "4. To take analogy with the real animal brain. synaptic nerves that connect with the neuron etc. (biology)\n",
    "\n",
    "5. Memory Model, Approximate dictionary.\n",
    "\n",
    "d = {}\n",
    "d[x] = y # set, training\n",
    "y1 = d[x] #get, inference\n",
    "\n",
    "--\n",
    "\n",
    "nn = Model_NN()\n",
    "nn[circumstance1] = result1 # training\n",
    "nn[circumstance2] = result2\n",
    "...\n",
    "...\n",
    "\n",
    "\n",
    "nn[new_circumstances] # prediction\n",
    "\n",
    "\n",
    "6. Lower layers are taking care of very raw features\n",
    "Think of how would you draw a forrest of trees.\n",
    "lines -> leaf -> branch of leafs -> tree -> forrest\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Sequential\n",
    "model=Sequential([\n",
    "    layers.Input(shape=(28, 28)),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(25, activation=\"relu\"),\n",
    "    layers.Dropout(0.4),\n",
    "    layers.Dense(25, activation=\"relu\"),\n",
    "    layers.Dropout(0.4),\n",
    "    layers.Dense(25, activation=\"relu\"),\n",
    "    layers.Dropout(0.4),\n",
    "    layers.Dense(25, activation=\"relu\"),\n",
    "    layers.Dropout(0.4),\n",
    "    layers.Dense(25, activation=\"relu\"),\n",
    "    layers.Dropout(0.4),\n",
    "    layers.Dense(25, activation=\"relu\"),\n",
    "    layers.Dropout(0.4),\n",
    "    layers.Dense(25, activation=\"relu\"),\n",
    "    layers.Dropout(0.4),\n",
    "    layers.Dense(25, activation=\"relu\"),\n",
    "    layers.Dropout(0.4),\n",
    "    layers.Dense(25, activation=\"relu\"),\n",
    "    layers.Dropout(0.4),\n",
    "    layers.Dense(25, activation=\"relu\"),\n",
    "    layers.Dropout(0.4),\n",
    "    layers.Dense(25, activation=\"relu\"),\n",
    "    layers.Dropout(0.4),\n",
    "    layers.Dense(25, activation=\"relu\"),\n",
    "    layers.Dropout(0.4),\n",
    "    layers.Dense(25, activation=\"relu\"),\n",
    "    layers.Dropout(0.4),\n",
    "    layers.Dense(25, activation=\"relu\"),\n",
    "    layers.Dropout(0.4),\n",
    "    layers.Dense(25, activation=\"relu\"),\n",
    "    layers.Dropout(0.4),\n",
    "    layers.Dense(25, activation=\"relu\"),\n",
    "    layers.Dropout(0.4),\n",
    "    layers.Dense(25, activation=\"relu\"),\n",
    "    layers.Dense(100000, activation=\"softmax\"),\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Transfer Learning - Borrowing from pretraining layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sequential\n",
    "model=Sequential([\n",
    "    layers.Input(shape=(28, 28)),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(25, activation=\"relu\"),\n",
    "    layers.Dropout(0.4),\n",
    "    layers.Dense(25, activation=\"relu\"),\n",
    "    layers.Dropout(0.4),\n",
    "    layers.Dense(25, activation=\"relu\"),\n",
    "    layers.Dropout(0.4),\n",
    "    layers.Dense(25, activation=\"relu\"),\n",
    "    layers.Dropout(0.4),\n",
    "    layers.Dense(25, activation=\"relu\"),\n",
    "    layers.Dropout(0.4),\n",
    "    layers.Dense(25, activation=\"relu\"),\n",
    "    layers.Dropout(0.4),\n",
    "    layers.Dense(25, activation=\"relu\"),\n",
    "    layers.Dropout(0.4),\n",
    "    layers.Dense(25, activation=\"relu\"),\n",
    "    layers.Dropout(0.4),\n",
    "    layers.Dense(25, activation=\"relu\"),\n",
    "    layers.Dropout(0.4),\n",
    "    layers.Dense(25, activation=\"relu\"),\n",
    "    layers.Dropout(0.4),\n",
    "    layers.Dense(25, activation=\"relu\"),\n",
    "    layers.Dropout(0.4),\n",
    "    layers.Dense(25, activation=\"relu\"),\n",
    "    layers.Dropout(0.4),\n",
    "    layers.Dense(25, activation=\"relu\"),\n",
    "    layers.Dropout(0.4),\n",
    "    layers.Dense(25, activation=\"relu\"),\n",
    "    layers.Dropout(0.4),\n",
    "    layers.Dense(25, activation=\"relu\"),\n",
    "    layers.Dropout(0.4),\n",
    "    layers.Dense(25, activation=\"relu\"),\n",
    "    layers.Dropout(0.4),\n",
    "    layers.Dense(25, activation=\"relu\"),\n",
    "    layers.Dense(1, activation=\"softmax\"),\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Embeddings size - 25\n",
    "\n",
    "    New Imageset X -> Server -> 25 numbers  X1 --> Model -> 0/1\n",
    "    model.fit(X1, y)\n",
    "\n",
    "    Definition\n",
    "\n",
    "    Embeddings\n",
    "    A meaningful features extracted by a complex model.\n",
    "    Unique characted produced by a trained neural network.\n",
    "\n",
    "    Languge --> LLM -> 1530 Features of the text.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the images of the planet. Unsupervised. AutoEncoder.\n",
    "\n",
    "Image --> NN ---> Image\n",
    "\n",
    "Constraint.\n",
    "\n",
    "1. Image ---> NN1 encoder --> 25 numbers ---> NN2 decoder --> Image\n",
    "\n",
    "Image --> NN (L1 l2(25)  l3...)\n",
    "\n",
    "2. Image + Noise --> NN --> Image\n",
    "\n",
    "3. Image Autocompletion - Self Supervised\n",
    "\n",
    "Incomplete Image --> NN -> Image\n",
    "\n",
    "4. Contrastive Learning\n",
    "\n",
    "Image1, Image2\n",
    "Image11\n",
    "\n",
    "Image1 ---> NN --> E1\n",
    "Image2 ---> NN --> E2\n",
    "Image11 ---> NN --> E11\n",
    "\n",
    "Loss((E1 - E11) / (E1 - E2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Query: Something to wear on head in winters - QE\n",
    "Products:\n",
    "    - Woolean hats - E1\n",
    "    - Woolean caps - E2\n",
    "    - winter monkey cap - E3\n",
    "    - Shoes - E4\n",
    "    - shirt - E5\n",
    "    - shorts - E6\n",
    "    ...\n",
    "   100s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_nearest_product(query):\n",
    "    min_dist = 1000\n",
    "    qe = embedding(query)\n",
    "    best_match = None\n",
    "    for p in products: # O(n)\n",
    "        e = embedding(p) # save it in some memory db such as redis, memcache\n",
    "        dist = eucl_distance(qe, e)\n",
    "        if dist < min_dist:\n",
    "            min_dist = dist\n",
    "            best_match = p\n",
    "    return best_match"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    # Vector Stores - Indexing\n",
    "\n",
    "    A million vectors\n",
    "    -> Group them in 10 groups using Kmeans. Each 100k\n",
    "    -->>> Group them into 10 groups. Each 10K\n",
    "    --------> 1000\n",
    "    ------------> 100\n",
    "    ---------------> 10\n",
    "    \n",
    "    # Lookup\n",
    "        - compute the distance between qe and 10 centers of group at top level. Pick the group closest.\n",
    "            - pick the next 10 childred of the group and repeat the same process\n",
    "                - ...\n",
    "\n",
    "    # Vector Data Bases -> \n",
    "        - Milvus\n",
    "        - Pine Cone\n",
    "        - Weaviate\n",
    "        - faiss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ex1 - Create a good database of product descriptions. Use Univ Sentence encoder to create embeddings and using nearest neight approach (find_nearest_product) to search for a product.\n",
    "\n",
    "Ex2 - Experiment with Milvus and USE to do the product lookup. Compare the performance of Ex1 and Ex2.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- If our data is linear separable, then usual linear regression.\n",
    "- But linreg doesn't work the moment we got taxation data. We needed deep layer\n",
    "- IN real world such as image recognition, the problems are non-linear. You need many deep layers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Humming bird doesn't have too many deep layer but has wider nn because it can see more color."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10% man are colorblind - R/G, B/V. 0% woman are color blind."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bigger brain, sleep more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bigger brain has tendency to memorize more and hence figure out patterns less"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"california_housing_dnn\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "features (InputLayer)        [(None, 8)]               0         \n",
      "_________________________________________________________________\n",
      "normalization_2 (Normalizati (None, 8)                 17        \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 50)                450       \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 50)                200       \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 30)                1530      \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch (None, 30)                120       \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 30)                0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 30)                930       \n",
      "_________________________________________________________________\n",
      "batch_normalization_9 (Batch (None, 30)                120       \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 30)                0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 30)                930       \n",
      "_________________________________________________________________\n",
      "batch_normalization_10 (Batc (None, 30)                120       \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 30)                0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 10)                310       \n",
      "_________________________________________________________________\n",
      "batch_normalization_11 (Batc (None, 10)                40        \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "price (Dense)                (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 4,778\n",
      "Trainable params: 4,461\n",
      "Non-trainable params: 317\n",
      "_________________________________________________________________\n",
      "Epoch 1/200\n",
      "194/194 - 3s - loss: 3.9861 - mae: 1.6521 - rmse: 1.9965 - val_loss: 1.8878 - val_mae: 1.1228 - val_rmse: 1.3740\n",
      "Epoch 2/200\n",
      "194/194 - 1s - loss: 1.5843 - mae: 0.9602 - rmse: 1.2587 - val_loss: 0.6792 - val_mae: 0.5748 - val_rmse: 0.8241\n",
      "Epoch 3/200\n",
      "194/194 - 1s - loss: 0.9703 - mae: 0.7365 - rmse: 0.9850 - val_loss: 0.5493 - val_mae: 0.5283 - val_rmse: 0.7412\n",
      "Epoch 4/200\n",
      "194/194 - 1s - loss: 0.8302 - mae: 0.6852 - rmse: 0.9112 - val_loss: 0.5138 - val_mae: 0.5101 - val_rmse: 0.7168\n",
      "Epoch 5/200\n",
      "194/194 - 1s - loss: 0.7641 - mae: 0.6528 - rmse: 0.8741 - val_loss: 0.4897 - val_mae: 0.5029 - val_rmse: 0.6998\n",
      "Epoch 6/200\n",
      "194/194 - 1s - loss: 0.7133 - mae: 0.6282 - rmse: 0.8446 - val_loss: 0.4767 - val_mae: 0.4902 - val_rmse: 0.6904\n",
      "Epoch 7/200\n",
      "194/194 - 2s - loss: 0.6563 - mae: 0.6003 - rmse: 0.8101 - val_loss: 0.4560 - val_mae: 0.4864 - val_rmse: 0.6753\n",
      "Epoch 8/200\n",
      "194/194 - 1s - loss: 0.6151 - mae: 0.5780 - rmse: 0.7843 - val_loss: 0.4413 - val_mae: 0.4765 - val_rmse: 0.6643\n",
      "Epoch 9/200\n",
      "194/194 - 1s - loss: 0.5856 - mae: 0.5639 - rmse: 0.7652 - val_loss: 0.4284 - val_mae: 0.4688 - val_rmse: 0.6546\n",
      "Epoch 10/200\n",
      "194/194 - 1s - loss: 0.5595 - mae: 0.5496 - rmse: 0.7480 - val_loss: 0.4184 - val_mae: 0.4661 - val_rmse: 0.6468\n",
      "Epoch 11/200\n",
      "194/194 - 1s - loss: 0.5373 - mae: 0.5364 - rmse: 0.7330 - val_loss: 0.4144 - val_mae: 0.4631 - val_rmse: 0.6437\n",
      "Epoch 12/200\n",
      "194/194 - 1s - loss: 0.5266 - mae: 0.5285 - rmse: 0.7257 - val_loss: 0.3996 - val_mae: 0.4515 - val_rmse: 0.6322\n",
      "Epoch 13/200\n",
      "194/194 - 1s - loss: 0.5078 - mae: 0.5180 - rmse: 0.7126 - val_loss: 0.3942 - val_mae: 0.4517 - val_rmse: 0.6278\n",
      "Epoch 14/200\n",
      "194/194 - 1s - loss: 0.4839 - mae: 0.5080 - rmse: 0.6956 - val_loss: 0.3833 - val_mae: 0.4379 - val_rmse: 0.6191\n",
      "Epoch 15/200\n",
      "194/194 - 1s - loss: 0.4796 - mae: 0.5020 - rmse: 0.6925 - val_loss: 0.3783 - val_mae: 0.4361 - val_rmse: 0.6150\n",
      "Epoch 16/200\n",
      "194/194 - 1s - loss: 0.4664 - mae: 0.4972 - rmse: 0.6829 - val_loss: 0.3761 - val_mae: 0.4353 - val_rmse: 0.6133\n",
      "Epoch 17/200\n",
      "194/194 - 1s - loss: 0.4587 - mae: 0.4936 - rmse: 0.6773 - val_loss: 0.3774 - val_mae: 0.4317 - val_rmse: 0.6143\n",
      "Epoch 18/200\n",
      "194/194 - 1s - loss: 0.4513 - mae: 0.4861 - rmse: 0.6718 - val_loss: 0.3759 - val_mae: 0.4355 - val_rmse: 0.6131\n",
      "Epoch 19/200\n",
      "194/194 - 1s - loss: 0.4520 - mae: 0.4872 - rmse: 0.6723 - val_loss: 0.3699 - val_mae: 0.4325 - val_rmse: 0.6082\n",
      "Epoch 20/200\n",
      "194/194 - 1s - loss: 0.4399 - mae: 0.4781 - rmse: 0.6633 - val_loss: 0.3577 - val_mae: 0.4230 - val_rmse: 0.5981\n",
      "Epoch 21/200\n",
      "194/194 - 1s - loss: 0.4318 - mae: 0.4755 - rmse: 0.6571 - val_loss: 0.3586 - val_mae: 0.4276 - val_rmse: 0.5988\n",
      "Epoch 22/200\n",
      "194/194 - 1s - loss: 0.4291 - mae: 0.4746 - rmse: 0.6550 - val_loss: 0.3632 - val_mae: 0.4254 - val_rmse: 0.6026\n",
      "Epoch 23/200\n",
      "194/194 - 1s - loss: 0.4211 - mae: 0.4677 - rmse: 0.6489 - val_loss: 0.3546 - val_mae: 0.4289 - val_rmse: 0.5955\n",
      "Epoch 24/200\n",
      "194/194 - 1s - loss: 0.4292 - mae: 0.4724 - rmse: 0.6551 - val_loss: 0.3562 - val_mae: 0.4206 - val_rmse: 0.5968\n",
      "Epoch 25/200\n",
      "194/194 - 1s - loss: 0.4133 - mae: 0.4611 - rmse: 0.6429 - val_loss: 0.3473 - val_mae: 0.4214 - val_rmse: 0.5894\n",
      "Epoch 26/200\n",
      "194/194 - 1s - loss: 0.4130 - mae: 0.4622 - rmse: 0.6426 - val_loss: 0.3479 - val_mae: 0.4175 - val_rmse: 0.5898\n",
      "Epoch 27/200\n",
      "194/194 - 1s - loss: 0.4077 - mae: 0.4575 - rmse: 0.6385 - val_loss: 0.3477 - val_mae: 0.4216 - val_rmse: 0.5897\n",
      "Epoch 28/200\n",
      "194/194 - 1s - loss: 0.4128 - mae: 0.4628 - rmse: 0.6425 - val_loss: 0.3444 - val_mae: 0.4175 - val_rmse: 0.5869\n",
      "Epoch 29/200\n",
      "194/194 - 1s - loss: 0.4157 - mae: 0.4589 - rmse: 0.6448 - val_loss: 0.3471 - val_mae: 0.4154 - val_rmse: 0.5891\n",
      "Epoch 30/200\n",
      "194/194 - 1s - loss: 0.4044 - mae: 0.4553 - rmse: 0.6359 - val_loss: 0.3570 - val_mae: 0.4236 - val_rmse: 0.5975\n",
      "Epoch 31/200\n",
      "194/194 - 1s - loss: 0.4021 - mae: 0.4528 - rmse: 0.6341 - val_loss: 0.3456 - val_mae: 0.4271 - val_rmse: 0.5879\n",
      "Epoch 32/200\n",
      "194/194 - 1s - loss: 0.4022 - mae: 0.4526 - rmse: 0.6342 - val_loss: 0.3373 - val_mae: 0.4141 - val_rmse: 0.5807\n",
      "Epoch 33/200\n",
      "194/194 - 1s - loss: 0.3983 - mae: 0.4481 - rmse: 0.6311 - val_loss: 0.3369 - val_mae: 0.4127 - val_rmse: 0.5805\n",
      "Epoch 34/200\n",
      "194/194 - 1s - loss: 0.3920 - mae: 0.4479 - rmse: 0.6261 - val_loss: 0.3463 - val_mae: 0.4190 - val_rmse: 0.5885\n",
      "Epoch 35/200\n",
      "194/194 - 1s - loss: 0.4004 - mae: 0.4534 - rmse: 0.6328 - val_loss: 0.3571 - val_mae: 0.4193 - val_rmse: 0.5976\n",
      "Epoch 36/200\n",
      "194/194 - 1s - loss: 0.3947 - mae: 0.4476 - rmse: 0.6283 - val_loss: 0.3434 - val_mae: 0.4194 - val_rmse: 0.5860\n",
      "Epoch 37/200\n",
      "194/194 - 1s - loss: 0.3860 - mae: 0.4454 - rmse: 0.6213 - val_loss: 0.3564 - val_mae: 0.4206 - val_rmse: 0.5970\n",
      "Epoch 38/200\n",
      "194/194 - 1s - loss: 0.3821 - mae: 0.4375 - rmse: 0.6182 - val_loss: 0.3458 - val_mae: 0.4217 - val_rmse: 0.5881\n",
      "\n",
      "Epoch 00038: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "Epoch 39/200\n",
      "194/194 - 1s - loss: 0.3828 - mae: 0.4394 - rmse: 0.6187 - val_loss: 0.3403 - val_mae: 0.4191 - val_rmse: 0.5833\n",
      "Epoch 40/200\n",
      "194/194 - 1s - loss: 0.3775 - mae: 0.4373 - rmse: 0.6145 - val_loss: 0.3406 - val_mae: 0.4173 - val_rmse: 0.5836\n",
      "Epoch 41/200\n",
      "194/194 - 1s - loss: 0.3804 - mae: 0.4399 - rmse: 0.6168 - val_loss: 0.3384 - val_mae: 0.4160 - val_rmse: 0.5818\n",
      "Epoch 42/200\n",
      "194/194 - 1s - loss: 0.3843 - mae: 0.4418 - rmse: 0.6199 - val_loss: 0.3471 - val_mae: 0.4215 - val_rmse: 0.5892\n",
      "Epoch 43/200\n",
      "194/194 - 1s - loss: 0.3808 - mae: 0.4403 - rmse: 0.6171 - val_loss: 0.3390 - val_mae: 0.4188 - val_rmse: 0.5823\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00043: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\n",
      "Test metrics:\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.3335 - mae: 0.4077 - rmse: 0.5775\n",
      "loss: 0.3335\n",
      "mae: 0.4077\n",
      "rmse: 0.5775\n",
      "\n",
      "Sample predictions vs actual:\n",
      "00  Pred: 1.214  Actual: 0.413 (units are ~$100k)\n",
      "01  Pred: 0.809  Actual: 0.690 (units are ~$100k)\n",
      "02  Pred: 2.100  Actual: 1.675 (units are ~$100k)\n",
      "03  Pred: 2.132  Actual: 2.359 (units are ~$100k)\n",
      "04  Pred: 1.062  Actual: 0.584 (units are ~$100k)\n"
     ]
    }
   ],
   "source": [
    "# Fully connected DNN for California Housing (Regression)\n",
    "# - Uses Keras Functional API (no CNNs)\n",
    "# - Includes normalization, early stopping, and learning-rate scheduling\n",
    "\n",
    "import os, random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# -------------------------\n",
    "# Reproducibility\n",
    "# -------------------------\n",
    "SEED = 42\n",
    "os.environ[\"PYTHONHASHSEED\"] = str(SEED)\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "\n",
    "# -------------------------\n",
    "# Load dataset\n",
    "# -------------------------\n",
    "data = fetch_california_housing()\n",
    "X = data.data.astype(\"float32\")   # Already NumPy\n",
    "y = data.target.astype(\"float32\") # Already NumPy\n",
    "\n",
    "# Train/Val/Test split (60/20/20)\n",
    "X_train, X_tmp, y_train, y_tmp = train_test_split(X, y, test_size=0.4, random_state=SEED)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_tmp, y_tmp, test_size=0.5, random_state=SEED)\n",
    "\n",
    "# -------------------------\n",
    "# tf.data pipelines\n",
    "# -------------------------\n",
    "BATCH = 64\n",
    "\n",
    "def make_ds(features, labels, training=False):\n",
    "    ds = tf.data.Dataset.from_tensor_slices((features, labels))\n",
    "    if training:\n",
    "        ds = ds.shuffle(buffer_size=len(features), seed=SEED)\n",
    "    return ds.batch(BATCH).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "train_ds = make_ds(X_train, y_train, training=True)\n",
    "val_ds   = make_ds(X_val, y_val)\n",
    "test_ds  = make_ds(X_test, y_test)\n",
    "\n",
    "# -------------------------\n",
    "# Normalization layer (learns from train data only)\n",
    "# -------------------------\n",
    "norm = tf.keras.layers.Normalization(axis=-1)\n",
    "norm.adapt(X_train)\n",
    "\n",
    "# -------------------------\n",
    "# Build the DNN (Functional API)\n",
    "# -------------------------\n",
    "inputs = tf.keras.Input(shape=(X.shape[1],), name=\"features\")\n",
    "x = norm(inputs)\n",
    "\n",
    "# A solid baseline: deep, fully connected with dropout & batch norm\n",
    "for units in [50, 30, 30, 30, 10]: #256, 128, 64, 32]:\n",
    "    x = tf.keras.layers.Dense(units, activation=\"relu\", kernel_initializer=\"he_normal\")(x)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = tf.keras.layers.Dropout(0.1)(x)\n",
    "\n",
    "outputs = tf.keras.layers.Dense(1, activation=\"linear\", name=\"price\")(x)\n",
    "model = tf.keras.Model(inputs, outputs, name=\"california_housing_dnn\")\n",
    "\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\n",
    "    loss=\"mse\",\n",
    "    metrics=[tf.keras.metrics.MeanAbsoluteError(name=\"mae\"),\n",
    "             tf.keras.metrics.RootMeanSquaredError(name=\"rmse\")]\n",
    ")\n",
    "\n",
    "model.summary()\n",
    "\n",
    "# -------------------------\n",
    "# Train\n",
    "# -------------------------\n",
    "callbacks = [\n",
    "    tf.keras.callbacks.EarlyStopping(\n",
    "        monitor=\"val_loss\", patience=10, restore_best_weights=True\n",
    "    ),\n",
    "    tf.keras.callbacks.ReduceLROnPlateau(\n",
    "        monitor=\"val_loss\", factor=0.5, patience=5, min_lr=1e-6, verbose=1\n",
    "    )\n",
    "]\n",
    "\n",
    "history = model.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    epochs=200,\n",
    "    callbacks=callbacks,\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "# -------------------------\n",
    "# Evaluate\n",
    "# -------------------------\n",
    "print(\"\\nTest metrics:\")\n",
    "test_metrics = model.evaluate(test_ds, return_dict=True)\n",
    "for k, v in test_metrics.items():\n",
    "    print(f\"{k}: {v:.4f}\")\n",
    "\n",
    "# -------------------------\n",
    "# Predict a few samples\n",
    "# -------------------------\n",
    "pred = model.predict(X_test[:5])\n",
    "print(\"\\nSample predictions vs actual:\")\n",
    "for i, (p, a) in enumerate(zip(pred.flatten(), y_test[:5])):\n",
    "    print(f\"{i:02d}  Pred: {p:.3f}  Actual: {a:.3f} (units are ~$100k)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2000\n",
      "194/194 - 1s - loss: 0.3898 - mae: 0.4462 - rmse: 0.6243 - val_loss: 0.3365 - val_mae: 0.4140 - val_rmse: 0.5801\n",
      "Epoch 2/2000\n",
      "194/194 - 1s - loss: 0.3860 - mae: 0.4435 - rmse: 0.6213 - val_loss: 0.3427 - val_mae: 0.4196 - val_rmse: 0.5854\n",
      "Epoch 3/2000\n",
      "194/194 - 1s - loss: 0.3899 - mae: 0.4436 - rmse: 0.6244 - val_loss: 0.3410 - val_mae: 0.4190 - val_rmse: 0.5840\n",
      "Epoch 4/2000\n",
      "194/194 - 1s - loss: 0.3818 - mae: 0.4409 - rmse: 0.6179 - val_loss: 0.3426 - val_mae: 0.4185 - val_rmse: 0.5853\n",
      "Epoch 5/2000\n",
      "194/194 - 1s - loss: 0.3905 - mae: 0.4474 - rmse: 0.6249 - val_loss: 0.3383 - val_mae: 0.4158 - val_rmse: 0.5817\n",
      "Epoch 6/2000\n",
      "194/194 - 1s - loss: 0.3918 - mae: 0.4450 - rmse: 0.6260 - val_loss: 0.3424 - val_mae: 0.4196 - val_rmse: 0.5852\n",
      "\n",
      "Epoch 00006: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "Epoch 7/2000\n",
      "194/194 - 1s - loss: 0.3854 - mae: 0.4429 - rmse: 0.6208 - val_loss: 0.3371 - val_mae: 0.4153 - val_rmse: 0.5806\n",
      "Epoch 8/2000\n",
      "194/194 - 1s - loss: 0.3840 - mae: 0.4423 - rmse: 0.6197 - val_loss: 0.3397 - val_mae: 0.4168 - val_rmse: 0.5828\n",
      "Epoch 9/2000\n",
      "194/194 - 1s - loss: 0.3807 - mae: 0.4424 - rmse: 0.6170 - val_loss: 0.3382 - val_mae: 0.4153 - val_rmse: 0.5816\n",
      "Epoch 10/2000\n",
      "194/194 - 1s - loss: 0.3825 - mae: 0.4437 - rmse: 0.6185 - val_loss: 0.3440 - val_mae: 0.4207 - val_rmse: 0.5865\n",
      "Epoch 11/2000\n",
      "194/194 - 1s - loss: 0.3936 - mae: 0.4478 - rmse: 0.6274 - val_loss: 0.3396 - val_mae: 0.4175 - val_rmse: 0.5827\n",
      "\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
      "\n",
      "Test metrics:\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.3324 - mae: 0.4084 - rmse: 0.5766\n",
      "loss: 0.3324\n",
      "mae: 0.4084\n",
      "rmse: 0.5766\n",
      "\n",
      "Sample predictions vs actual:\n",
      "00  Pred: 1.174  Actual: 0.413 (units are ~$100k)\n",
      "01  Pred: 0.842  Actual: 0.690 (units are ~$100k)\n",
      "02  Pred: 2.114  Actual: 1.675 (units are ~$100k)\n",
      "03  Pred: 2.100  Actual: 2.359 (units are ~$100k)\n",
      "04  Pred: 1.056  Actual: 0.584 (units are ~$100k)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "history = model.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    epochs=2000,\n",
    "    callbacks=callbacks,\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "# -------------------------\n",
    "# Evaluate\n",
    "# -------------------------\n",
    "print(\"\\nTest metrics:\")\n",
    "test_metrics = model.evaluate(test_ds, return_dict=True)\n",
    "for k, v in test_metrics.items():\n",
    "    print(f\"{k}: {v:.4f}\")\n",
    "\n",
    "# -------------------------\n",
    "# Predict a few samples\n",
    "# -------------------------\n",
    "pred = model.predict(X_test[:5])\n",
    "print(\"\\nSample predictions vs actual:\")\n",
    "for i, (p, a) in enumerate(zip(pred.flatten(), y_test[:5])):\n",
    "    print(f\"{i:02d}  Pred: {p:.3f}  Actual: {a:.3f} (units are ~$100k)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "194/194 - 2s - loss: 0.2590 - mae_real: 10234.3662 - rmse_real: 81865.4453 - val_loss: 0.1698 - val_mae_real: 0.6651 - val_rmse_real: 1.0301\n",
      "Epoch 2/200\n",
      "194/194 - 1s - loss: 0.1688 - mae_real: 2027.7018 - rmse_real: 11546.3633 - val_loss: 0.1571 - val_mae_real: 0.5929 - val_rmse_real: 0.8629\n",
      "Epoch 3/200\n",
      "194/194 - 1s - loss: 0.1562 - mae_real: 0.6835 - rmse_real: 1.5435 - val_loss: 0.1394 - val_mae_real: 0.4927 - val_rmse_real: 0.7532\n",
      "Epoch 4/200\n",
      "194/194 - 1s - loss: 0.1399 - mae_real: 0.5590 - rmse_real: 0.9865 - val_loss: 0.1323 - val_mae_real: 0.4830 - val_rmse_real: 0.6825\n",
      "Epoch 5/200\n",
      "194/194 - 1s - loss: 0.1319 - mae_real: 0.5270 - rmse_real: 0.9113 - val_loss: 0.1260 - val_mae_real: 0.4865 - val_rmse_real: 0.6844\n",
      "Epoch 6/200\n",
      "194/194 - 1s - loss: 0.1264 - mae_real: 0.5866 - rmse_real: 1.4016 - val_loss: 0.1192 - val_mae_real: 0.4587 - val_rmse_real: 0.6767\n",
      "Epoch 7/200\n",
      "194/194 - 1s - loss: 0.1186 - mae_real: 0.4857 - rmse_real: 0.7062 - val_loss: 0.1127 - val_mae_real: 0.4529 - val_rmse_real: 0.6633\n",
      "Epoch 8/200\n",
      "194/194 - 1s - loss: 0.1113 - mae_real: 0.4703 - rmse_real: 0.6804 - val_loss: 0.1088 - val_mae_real: 0.4705 - val_rmse_real: 0.6970\n",
      "Epoch 9/200\n",
      "194/194 - 1s - loss: 0.1064 - mae_real: 0.4925 - rmse_real: 0.8831 - val_loss: 0.1005 - val_mae_real: 0.4314 - val_rmse_real: 0.6316\n",
      "Epoch 10/200\n",
      "194/194 - 1s - loss: 0.0996 - mae_real: 0.4695 - rmse_real: 0.7649 - val_loss: 0.0954 - val_mae_real: 0.4361 - val_rmse_real: 0.6352\n",
      "Epoch 11/200\n",
      "194/194 - 1s - loss: 0.0938 - mae_real: 0.5002 - rmse_real: 1.0620 - val_loss: 0.0922 - val_mae_real: 0.4534 - val_rmse_real: 0.6736\n",
      "Epoch 12/200\n",
      "194/194 - 1s - loss: 0.0880 - mae_real: 0.4414 - rmse_real: 0.6375 - val_loss: 0.0849 - val_mae_real: 0.4284 - val_rmse_real: 0.6280\n",
      "Epoch 13/200\n",
      "194/194 - 1s - loss: 0.0831 - mae_real: 0.4386 - rmse_real: 0.6481 - val_loss: 0.0794 - val_mae_real: 0.4165 - val_rmse_real: 0.6132\n",
      "Epoch 14/200\n",
      "194/194 - 1s - loss: 0.0774 - mae_real: 0.4368 - rmse_real: 0.6870 - val_loss: 0.0748 - val_mae_real: 0.4194 - val_rmse_real: 0.6091\n",
      "Epoch 15/200\n",
      "194/194 - 1s - loss: 0.0733 - mae_real: 0.4326 - rmse_real: 0.6660 - val_loss: 0.0703 - val_mae_real: 0.4197 - val_rmse_real: 0.6064\n",
      "Epoch 16/200\n",
      "194/194 - 1s - loss: 0.0683 - mae_real: 0.4259 - rmse_real: 0.6405 - val_loss: 0.0654 - val_mae_real: 0.4071 - val_rmse_real: 0.5892\n",
      "Epoch 17/200\n",
      "194/194 - 1s - loss: 0.0637 - mae_real: 0.4148 - rmse_real: 0.6054 - val_loss: 0.0618 - val_mae_real: 0.4126 - val_rmse_real: 0.5986\n",
      "Epoch 18/200\n",
      "194/194 - 1s - loss: 0.0597 - mae_real: 0.4126 - rmse_real: 0.5968 - val_loss: 0.0573 - val_mae_real: 0.3973 - val_rmse_real: 0.5922\n",
      "Epoch 19/200\n",
      "194/194 - 1s - loss: 0.0554 - mae_real: 0.4061 - rmse_real: 0.5916 - val_loss: 0.0539 - val_mae_real: 0.4072 - val_rmse_real: 0.5883\n",
      "Epoch 20/200\n",
      "194/194 - 1s - loss: 0.0523 - mae_real: 0.4101 - rmse_real: 0.6144 - val_loss: 0.0505 - val_mae_real: 0.4074 - val_rmse_real: 0.5904\n",
      "Epoch 21/200\n",
      "194/194 - 1s - loss: 0.0483 - mae_real: 0.3988 - rmse_real: 0.5763 - val_loss: 0.0469 - val_mae_real: 0.3955 - val_rmse_real: 0.5824\n",
      "Epoch 22/200\n",
      "194/194 - 1s - loss: 0.0459 - mae_real: 0.4002 - rmse_real: 0.5808 - val_loss: 0.0441 - val_mae_real: 0.3953 - val_rmse_real: 0.5821\n",
      "Epoch 23/200\n",
      "194/194 - 1s - loss: 0.0426 - mae_real: 0.3951 - rmse_real: 0.5822 - val_loss: 0.0417 - val_mae_real: 0.4011 - val_rmse_real: 0.5839\n",
      "Epoch 24/200\n",
      "194/194 - 1s - loss: 0.0399 - mae_real: 0.4292 - rmse_real: 0.8596 - val_loss: 0.0393 - val_mae_real: 0.3996 - val_rmse_real: 0.5925\n",
      "Epoch 25/200\n",
      "194/194 - 1s - loss: 0.0366 - mae_real: 0.3863 - rmse_real: 0.5622 - val_loss: 0.0358 - val_mae_real: 0.3936 - val_rmse_real: 0.5645\n",
      "Epoch 26/200\n",
      "194/194 - 1s - loss: 0.0345 - mae_real: 0.3875 - rmse_real: 0.5738 - val_loss: 0.0342 - val_mae_real: 0.3910 - val_rmse_real: 0.5842\n",
      "Epoch 27/200\n",
      "194/194 - 1s - loss: 0.0322 - mae_real: 0.3858 - rmse_real: 0.5614 - val_loss: 0.0326 - val_mae_real: 0.3988 - val_rmse_real: 0.5840\n",
      "Epoch 28/200\n",
      "194/194 - 1s - loss: 0.0303 - mae_real: 0.3844 - rmse_real: 0.5877 - val_loss: 0.0301 - val_mae_real: 0.3872 - val_rmse_real: 0.5807\n",
      "Epoch 29/200\n",
      "194/194 - 1s - loss: 0.0280 - mae_real: 0.3769 - rmse_real: 0.5471 - val_loss: 0.0282 - val_mae_real: 0.3827 - val_rmse_real: 0.5679\n",
      "Epoch 30/200\n",
      "194/194 - 1s - loss: 0.0268 - mae_real: 0.3821 - rmse_real: 0.5605 - val_loss: 0.0266 - val_mae_real: 0.3825 - val_rmse_real: 0.5663\n",
      "Epoch 31/200\n",
      "194/194 - 1s - loss: 0.0249 - mae_real: 0.3712 - rmse_real: 0.5435 - val_loss: 0.0250 - val_mae_real: 0.3835 - val_rmse_real: 0.5512\n",
      "Epoch 32/200\n",
      "194/194 - 1s - loss: 0.0240 - mae_real: 0.3769 - rmse_real: 0.5766 - val_loss: 0.0240 - val_mae_real: 0.3773 - val_rmse_real: 0.5524\n",
      "Epoch 33/200\n",
      "194/194 - 1s - loss: 0.0225 - mae_real: 0.3684 - rmse_real: 0.5370 - val_loss: 0.0227 - val_mae_real: 0.3826 - val_rmse_real: 0.5477\n",
      "Epoch 34/200\n",
      "194/194 - 1s - loss: 0.0215 - mae_real: 0.3684 - rmse_real: 0.5407 - val_loss: 0.0220 - val_mae_real: 0.3770 - val_rmse_real: 0.5551\n",
      "Epoch 35/200\n",
      "194/194 - 1s - loss: 0.0206 - mae_real: 0.3702 - rmse_real: 0.5474 - val_loss: 0.0207 - val_mae_real: 0.3775 - val_rmse_real: 0.5472\n",
      "Epoch 36/200\n",
      "194/194 - 1s - loss: 0.0199 - mae_real: 0.3702 - rmse_real: 0.5460 - val_loss: 0.0203 - val_mae_real: 0.3808 - val_rmse_real: 0.5652\n",
      "Epoch 37/200\n",
      "194/194 - 1s - loss: 0.0191 - mae_real: 0.3673 - rmse_real: 0.5372 - val_loss: 0.0204 - val_mae_real: 0.4014 - val_rmse_real: 0.5545\n",
      "Epoch 38/200\n",
      "194/194 - 1s - loss: 0.0184 - mae_real: 0.3656 - rmse_real: 0.5403 - val_loss: 0.0194 - val_mae_real: 0.3853 - val_rmse_real: 0.5459\n",
      "Epoch 39/200\n",
      "194/194 - 1s - loss: 0.0178 - mae_real: 0.3632 - rmse_real: 0.5296 - val_loss: 0.0185 - val_mae_real: 0.3788 - val_rmse_real: 0.5506\n",
      "Epoch 40/200\n",
      "194/194 - 1s - loss: 0.0174 - mae_real: 0.3646 - rmse_real: 0.5340 - val_loss: 0.0179 - val_mae_real: 0.3736 - val_rmse_real: 0.5507\n",
      "Epoch 41/200\n",
      "194/194 - 1s - loss: 0.0171 - mae_real: 0.3625 - rmse_real: 0.5309 - val_loss: 0.0180 - val_mae_real: 0.3746 - val_rmse_real: 0.5649\n",
      "Epoch 42/200\n",
      "194/194 - 1s - loss: 0.0165 - mae_real: 0.3612 - rmse_real: 0.5315 - val_loss: 0.0172 - val_mae_real: 0.3709 - val_rmse_real: 0.5416\n",
      "Epoch 43/200\n",
      "194/194 - 1s - loss: 0.0164 - mae_real: 0.3627 - rmse_real: 0.5305 - val_loss: 0.0169 - val_mae_real: 0.3685 - val_rmse_real: 0.5430\n",
      "Epoch 44/200\n",
      "194/194 - 1s - loss: 0.0160 - mae_real: 0.3587 - rmse_real: 0.5354 - val_loss: 0.0165 - val_mae_real: 0.3652 - val_rmse_real: 0.5423\n",
      "Epoch 45/200\n",
      "194/194 - 1s - loss: 0.0157 - mae_real: 0.3614 - rmse_real: 0.5286 - val_loss: 0.0164 - val_mae_real: 0.3721 - val_rmse_real: 0.5377\n",
      "Epoch 46/200\n",
      "194/194 - 1s - loss: 0.0155 - mae_real: 0.3588 - rmse_real: 0.5245 - val_loss: 0.0176 - val_mae_real: 0.3838 - val_rmse_real: 0.5745\n",
      "Epoch 47/200\n",
      "194/194 - 1s - loss: 0.0152 - mae_real: 0.3571 - rmse_real: 0.5237 - val_loss: 0.0166 - val_mae_real: 0.3848 - val_rmse_real: 0.5553\n",
      "Epoch 48/200\n",
      "194/194 - 1s - loss: 0.0150 - mae_real: 0.3573 - rmse_real: 0.5234 - val_loss: 0.0159 - val_mae_real: 0.3685 - val_rmse_real: 0.5428\n",
      "Epoch 49/200\n",
      "194/194 - 1s - loss: 0.0150 - mae_real: 0.3570 - rmse_real: 0.5247 - val_loss: 0.0162 - val_mae_real: 0.3774 - val_rmse_real: 0.5506\n",
      "Epoch 50/200\n",
      "194/194 - 1s - loss: 0.0149 - mae_real: 0.3561 - rmse_real: 0.5240 - val_loss: 0.0157 - val_mae_real: 0.3645 - val_rmse_real: 0.5456\n",
      "Epoch 51/200\n",
      "194/194 - 1s - loss: 0.0149 - mae_real: 0.3582 - rmse_real: 0.5287 - val_loss: 0.0166 - val_mae_real: 0.3982 - val_rmse_real: 0.5614\n",
      "Epoch 52/200\n",
      "194/194 - 1s - loss: 0.0147 - mae_real: 0.3572 - rmse_real: 0.5238 - val_loss: 0.0156 - val_mae_real: 0.3724 - val_rmse_real: 0.5496\n",
      "Epoch 53/200\n",
      "194/194 - 1s - loss: 0.0145 - mae_real: 0.3552 - rmse_real: 0.5226 - val_loss: 0.0157 - val_mae_real: 0.3714 - val_rmse_real: 0.5497\n",
      "Epoch 54/200\n",
      "194/194 - 1s - loss: 0.0144 - mae_real: 0.3554 - rmse_real: 0.5215 - val_loss: 0.0157 - val_mae_real: 0.3767 - val_rmse_real: 0.5510\n",
      "Epoch 55/200\n",
      "194/194 - 1s - loss: 0.0143 - mae_real: 0.3554 - rmse_real: 0.5217 - val_loss: 0.0155 - val_mae_real: 0.3642 - val_rmse_real: 0.5472\n",
      "Epoch 56/200\n",
      "194/194 - 1s - loss: 0.0143 - mae_real: 0.3534 - rmse_real: 0.5204 - val_loss: 0.0159 - val_mae_real: 0.3769 - val_rmse_real: 0.5715\n",
      "Epoch 57/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "194/194 - 2s - loss: 0.0141 - mae_real: 0.3536 - rmse_real: 0.5184 - val_loss: 0.0168 - val_mae_real: 0.4055 - val_rmse_real: 0.5660\n",
      "Epoch 58/200\n",
      "194/194 - 1s - loss: 0.0142 - mae_real: 0.3529 - rmse_real: 0.5259 - val_loss: 0.0152 - val_mae_real: 0.3638 - val_rmse_real: 0.5458\n",
      "Epoch 59/200\n",
      "194/194 - 1s - loss: 0.0141 - mae_real: 0.3538 - rmse_real: 0.5183 - val_loss: 0.0152 - val_mae_real: 0.3621 - val_rmse_real: 0.5456\n",
      "Epoch 60/200\n",
      "194/194 - 1s - loss: 0.0140 - mae_real: 0.3523 - rmse_real: 0.5213 - val_loss: 0.0149 - val_mae_real: 0.3651 - val_rmse_real: 0.5362\n",
      "Epoch 61/200\n",
      "194/194 - 1s - loss: 0.0140 - mae_real: 0.3533 - rmse_real: 0.5192 - val_loss: 0.0155 - val_mae_real: 0.3743 - val_rmse_real: 0.5550\n",
      "Epoch 62/200\n",
      "194/194 - 1s - loss: 0.0139 - mae_real: 0.3500 - rmse_real: 0.5142 - val_loss: 0.0147 - val_mae_real: 0.3624 - val_rmse_real: 0.5322\n",
      "Epoch 63/200\n",
      "194/194 - 1s - loss: 0.0138 - mae_real: 0.3499 - rmse_real: 0.5148 - val_loss: 0.0150 - val_mae_real: 0.3638 - val_rmse_real: 0.5413\n",
      "Epoch 64/200\n",
      "194/194 - 1s - loss: 0.0140 - mae_real: 0.3543 - rmse_real: 0.5205 - val_loss: 0.0149 - val_mae_real: 0.3658 - val_rmse_real: 0.5386\n",
      "Epoch 65/200\n",
      "194/194 - 1s - loss: 0.0137 - mae_real: 0.3481 - rmse_real: 0.5121 - val_loss: 0.0151 - val_mae_real: 0.3694 - val_rmse_real: 0.5498\n",
      "Epoch 66/200\n",
      "194/194 - 1s - loss: 0.0137 - mae_real: 0.3494 - rmse_real: 0.5158 - val_loss: 0.0148 - val_mae_real: 0.3649 - val_rmse_real: 0.5369\n",
      "Epoch 67/200\n",
      "194/194 - 1s - loss: 0.0137 - mae_real: 0.3499 - rmse_real: 0.5153 - val_loss: 0.0147 - val_mae_real: 0.3627 - val_rmse_real: 0.5396\n",
      "Epoch 68/200\n",
      "194/194 - 1s - loss: 0.0135 - mae_real: 0.3463 - rmse_real: 0.5098 - val_loss: 0.0149 - val_mae_real: 0.3676 - val_rmse_real: 0.5464\n",
      "\n",
      "Epoch 00068: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "Epoch 69/200\n",
      "194/194 - 1s - loss: 0.0133 - mae_real: 0.3427 - rmse_real: 0.5049 - val_loss: 0.0143 - val_mae_real: 0.3563 - val_rmse_real: 0.5263\n",
      "Epoch 70/200\n",
      "194/194 - 1s - loss: 0.0131 - mae_real: 0.3393 - rmse_real: 0.5001 - val_loss: 0.0143 - val_mae_real: 0.3537 - val_rmse_real: 0.5304\n",
      "Epoch 71/200\n",
      "194/194 - 1s - loss: 0.0130 - mae_real: 0.3376 - rmse_real: 0.4981 - val_loss: 0.0146 - val_mae_real: 0.3634 - val_rmse_real: 0.5470\n",
      "Epoch 72/200\n",
      "194/194 - 1s - loss: 0.0132 - mae_real: 0.3408 - rmse_real: 0.5042 - val_loss: 0.0143 - val_mae_real: 0.3551 - val_rmse_real: 0.5301\n",
      "Epoch 73/200\n",
      "194/194 - 1s - loss: 0.0132 - mae_real: 0.3417 - rmse_real: 0.5042 - val_loss: 0.0143 - val_mae_real: 0.3544 - val_rmse_real: 0.5300\n",
      "Epoch 74/200\n",
      "194/194 - 1s - loss: 0.0130 - mae_real: 0.3378 - rmse_real: 0.5010 - val_loss: 0.0149 - val_mae_real: 0.3654 - val_rmse_real: 0.5517\n",
      "Epoch 75/200\n",
      "194/194 - 1s - loss: 0.0131 - mae_real: 0.3399 - rmse_real: 0.5006 - val_loss: 0.0146 - val_mae_real: 0.3616 - val_rmse_real: 0.5390\n",
      "\n",
      "Epoch 00075: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "Epoch 76/200\n",
      "194/194 - 1s - loss: 0.0128 - mae_real: 0.3345 - rmse_real: 0.4955 - val_loss: 0.0143 - val_mae_real: 0.3532 - val_rmse_real: 0.5325\n",
      "Epoch 77/200\n",
      "194/194 - 1s - loss: 0.0127 - mae_real: 0.3330 - rmse_real: 0.4928 - val_loss: 0.0142 - val_mae_real: 0.3562 - val_rmse_real: 0.5255\n",
      "Epoch 78/200\n",
      "194/194 - 1s - loss: 0.0127 - mae_real: 0.3324 - rmse_real: 0.4904 - val_loss: 0.0141 - val_mae_real: 0.3527 - val_rmse_real: 0.5303\n",
      "Epoch 79/200\n",
      "194/194 - 1s - loss: 0.0127 - mae_real: 0.3325 - rmse_real: 0.4914 - val_loss: 0.0144 - val_mae_real: 0.3526 - val_rmse_real: 0.5348\n",
      "Epoch 80/200\n",
      "194/194 - 1s - loss: 0.0127 - mae_real: 0.3328 - rmse_real: 0.4932 - val_loss: 0.0142 - val_mae_real: 0.3563 - val_rmse_real: 0.5299\n",
      "Epoch 81/200\n",
      "194/194 - 1s - loss: 0.0127 - mae_real: 0.3331 - rmse_real: 0.4937 - val_loss: 0.0142 - val_mae_real: 0.3513 - val_rmse_real: 0.5313\n",
      "Epoch 82/200\n",
      "194/194 - 1s - loss: 0.0126 - mae_real: 0.3315 - rmse_real: 0.4912 - val_loss: 0.0142 - val_mae_real: 0.3548 - val_rmse_real: 0.5263\n",
      "Epoch 83/200\n",
      "194/194 - 1s - loss: 0.0126 - mae_real: 0.3312 - rmse_real: 0.4906 - val_loss: 0.0140 - val_mae_real: 0.3498 - val_rmse_real: 0.5258\n",
      "Epoch 84/200\n",
      "194/194 - 1s - loss: 0.0126 - mae_real: 0.3328 - rmse_real: 0.4907 - val_loss: 0.0143 - val_mae_real: 0.3583 - val_rmse_real: 0.5239\n",
      "Epoch 85/200\n",
      "194/194 - 1s - loss: 0.0125 - mae_real: 0.3300 - rmse_real: 0.4898 - val_loss: 0.0140 - val_mae_real: 0.3502 - val_rmse_real: 0.5250\n",
      "Epoch 86/200\n",
      "194/194 - 1s - loss: 0.0125 - mae_real: 0.3304 - rmse_real: 0.4888 - val_loss: 0.0141 - val_mae_real: 0.3510 - val_rmse_real: 0.5291\n",
      "Epoch 87/200\n",
      "194/194 - 1s - loss: 0.0125 - mae_real: 0.3305 - rmse_real: 0.4898 - val_loss: 0.0140 - val_mae_real: 0.3511 - val_rmse_real: 0.5235\n",
      "Epoch 88/200\n",
      "194/194 - 1s - loss: 0.0126 - mae_real: 0.3310 - rmse_real: 0.4925 - val_loss: 0.0142 - val_mae_real: 0.3504 - val_rmse_real: 0.5287\n",
      "Epoch 89/200\n",
      "194/194 - 1s - loss: 0.0125 - mae_real: 0.3297 - rmse_real: 0.4879 - val_loss: 0.0141 - val_mae_real: 0.3515 - val_rmse_real: 0.5239\n",
      "\n",
      "Epoch 00089: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "Epoch 90/200\n",
      "194/194 - 1s - loss: 0.0124 - mae_real: 0.3285 - rmse_real: 0.4869 - val_loss: 0.0140 - val_mae_real: 0.3485 - val_rmse_real: 0.5244\n",
      "Epoch 91/200\n",
      "194/194 - 1s - loss: 0.0123 - mae_real: 0.3275 - rmse_real: 0.4851 - val_loss: 0.0139 - val_mae_real: 0.3499 - val_rmse_real: 0.5228\n",
      "Epoch 92/200\n",
      "194/194 - 1s - loss: 0.0124 - mae_real: 0.3272 - rmse_real: 0.4855 - val_loss: 0.0140 - val_mae_real: 0.3527 - val_rmse_real: 0.5204\n",
      "Epoch 93/200\n",
      "194/194 - 1s - loss: 0.0123 - mae_real: 0.3271 - rmse_real: 0.4846 - val_loss: 0.0139 - val_mae_real: 0.3490 - val_rmse_real: 0.5230\n",
      "Epoch 94/200\n",
      "194/194 - 1s - loss: 0.0123 - mae_real: 0.3266 - rmse_real: 0.4847 - val_loss: 0.0140 - val_mae_real: 0.3493 - val_rmse_real: 0.5265\n",
      "Epoch 95/200\n",
      "194/194 - 1s - loss: 0.0123 - mae_real: 0.3260 - rmse_real: 0.4835 - val_loss: 0.0140 - val_mae_real: 0.3475 - val_rmse_real: 0.5243\n",
      "\n",
      "Epoch 00095: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "Epoch 96/200\n",
      "194/194 - 1s - loss: 0.0123 - mae_real: 0.3262 - rmse_real: 0.4834 - val_loss: 0.0139 - val_mae_real: 0.3479 - val_rmse_real: 0.5235\n",
      "Epoch 97/200\n",
      "194/194 - 1s - loss: 0.0123 - mae_real: 0.3259 - rmse_real: 0.4838 - val_loss: 0.0139 - val_mae_real: 0.3479 - val_rmse_real: 0.5213\n",
      "Epoch 98/200\n",
      "194/194 - 1s - loss: 0.0121 - mae_real: 0.3240 - rmse_real: 0.4796 - val_loss: 0.0140 - val_mae_real: 0.3511 - val_rmse_real: 0.5202\n",
      "Epoch 99/200\n",
      "194/194 - 1s - loss: 0.0122 - mae_real: 0.3242 - rmse_real: 0.4808 - val_loss: 0.0139 - val_mae_real: 0.3494 - val_rmse_real: 0.5196\n",
      "Epoch 100/200\n",
      "194/194 - 1s - loss: 0.0123 - mae_real: 0.3251 - rmse_real: 0.4832 - val_loss: 0.0139 - val_mae_real: 0.3494 - val_rmse_real: 0.5194\n",
      "Epoch 101/200\n",
      "194/194 - 1s - loss: 0.0122 - mae_real: 0.3243 - rmse_real: 0.4817 - val_loss: 0.0139 - val_mae_real: 0.3483 - val_rmse_real: 0.5185\n",
      "Epoch 102/200\n",
      "194/194 - 1s - loss: 0.0121 - mae_real: 0.3238 - rmse_real: 0.4808 - val_loss: 0.0138 - val_mae_real: 0.3483 - val_rmse_real: 0.5183\n",
      "Epoch 103/200\n",
      "194/194 - 1s - loss: 0.0122 - mae_real: 0.3240 - rmse_real: 0.4810 - val_loss: 0.0139 - val_mae_real: 0.3497 - val_rmse_real: 0.5194\n",
      "\n",
      "Epoch 00103: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "Epoch 104/200\n",
      "194/194 - 1s - loss: 0.0121 - mae_real: 0.3241 - rmse_real: 0.4790 - val_loss: 0.0139 - val_mae_real: 0.3482 - val_rmse_real: 0.5188\n",
      "Epoch 105/200\n",
      "194/194 - 1s - loss: 0.0121 - mae_real: 0.3231 - rmse_real: 0.4782 - val_loss: 0.0138 - val_mae_real: 0.3472 - val_rmse_real: 0.5189\n",
      "Epoch 106/200\n",
      "194/194 - 1s - loss: 0.0122 - mae_real: 0.3239 - rmse_real: 0.4801 - val_loss: 0.0139 - val_mae_real: 0.3489 - val_rmse_real: 0.5193\n",
      "Epoch 107/200\n",
      "194/194 - 1s - loss: 0.0121 - mae_real: 0.3238 - rmse_real: 0.4798 - val_loss: 0.0139 - val_mae_real: 0.3479 - val_rmse_real: 0.5192\n",
      "Epoch 108/200\n",
      "194/194 - 1s - loss: 0.0121 - mae_real: 0.3236 - rmse_real: 0.4789 - val_loss: 0.0139 - val_mae_real: 0.3468 - val_rmse_real: 0.5210\n",
      "Epoch 109/200\n",
      "194/194 - 1s - loss: 0.0121 - mae_real: 0.3237 - rmse_real: 0.4792 - val_loss: 0.0139 - val_mae_real: 0.3467 - val_rmse_real: 0.5203\n",
      "\n",
      "Epoch 00109: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
      "Epoch 110/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "194/194 - 1s - loss: 0.0121 - mae_real: 0.3231 - rmse_real: 0.4799 - val_loss: 0.0138 - val_mae_real: 0.3474 - val_rmse_real: 0.5193\n",
      "Epoch 111/200\n",
      "194/194 - 1s - loss: 0.0121 - mae_real: 0.3239 - rmse_real: 0.4799 - val_loss: 0.0138 - val_mae_real: 0.3469 - val_rmse_real: 0.5199\n",
      "Epoch 112/200\n",
      "194/194 - 1s - loss: 0.0122 - mae_real: 0.3238 - rmse_real: 0.4811 - val_loss: 0.0138 - val_mae_real: 0.3480 - val_rmse_real: 0.5186\n",
      "Epoch 113/200\n",
      "194/194 - 1s - loss: 0.0121 - mae_real: 0.3240 - rmse_real: 0.4802 - val_loss: 0.0138 - val_mae_real: 0.3468 - val_rmse_real: 0.5201\n",
      "Epoch 114/200\n",
      "194/194 - 1s - loss: 0.0121 - mae_real: 0.3232 - rmse_real: 0.4785 - val_loss: 0.0139 - val_mae_real: 0.3469 - val_rmse_real: 0.5206\n",
      "Epoch 115/200\n",
      "194/194 - 1s - loss: 0.0121 - mae_real: 0.3230 - rmse_real: 0.4814 - val_loss: 0.0139 - val_mae_real: 0.3486 - val_rmse_real: 0.5191\n",
      "\n",
      "Epoch 00115: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n",
      "Epoch 116/200\n",
      "194/194 - 1s - loss: 0.0121 - mae_real: 0.3225 - rmse_real: 0.4764 - val_loss: 0.0138 - val_mae_real: 0.3469 - val_rmse_real: 0.5201\n",
      "Epoch 117/200\n",
      "194/194 - 1s - loss: 0.0121 - mae_real: 0.3226 - rmse_real: 0.4800 - val_loss: 0.0138 - val_mae_real: 0.3472 - val_rmse_real: 0.5195\n",
      "Epoch 118/200\n",
      "194/194 - 1s - loss: 0.0121 - mae_real: 0.3225 - rmse_real: 0.4789 - val_loss: 0.0138 - val_mae_real: 0.3473 - val_rmse_real: 0.5193\n",
      "Epoch 119/200\n",
      "194/194 - 1s - loss: 0.0121 - mae_real: 0.3227 - rmse_real: 0.4784 - val_loss: 0.0138 - val_mae_real: 0.3471 - val_rmse_real: 0.5196\n",
      "Epoch 120/200\n",
      "194/194 - 1s - loss: 0.0121 - mae_real: 0.3230 - rmse_real: 0.4793 - val_loss: 0.0138 - val_mae_real: 0.3471 - val_rmse_real: 0.5196\n",
      "Epoch 121/200\n",
      "194/194 - 1s - loss: 0.0121 - mae_real: 0.3229 - rmse_real: 0.4768 - val_loss: 0.0138 - val_mae_real: 0.3473 - val_rmse_real: 0.5194\n",
      "\n",
      "Epoch 00121: ReduceLROnPlateau reducing learning rate to 3.906250185536919e-06.\n",
      "Epoch 122/200\n",
      "194/194 - 1s - loss: 0.0121 - mae_real: 0.3225 - rmse_real: 0.4782 - val_loss: 0.0138 - val_mae_real: 0.3470 - val_rmse_real: 0.5197\n",
      "Epoch 123/200\n",
      "194/194 - 1s - loss: 0.0121 - mae_real: 0.3235 - rmse_real: 0.4801 - val_loss: 0.0138 - val_mae_real: 0.3473 - val_rmse_real: 0.5194\n",
      "Epoch 124/200\n",
      "194/194 - 1s - loss: 0.0121 - mae_real: 0.3227 - rmse_real: 0.4786 - val_loss: 0.0138 - val_mae_real: 0.3471 - val_rmse_real: 0.5196\n",
      "Epoch 125/200\n",
      "194/194 - 1s - loss: 0.0121 - mae_real: 0.3233 - rmse_real: 0.4790 - val_loss: 0.0138 - val_mae_real: 0.3472 - val_rmse_real: 0.5195\n",
      "Epoch 126/200\n",
      "194/194 - 1s - loss: 0.0121 - mae_real: 0.3226 - rmse_real: 0.4783 - val_loss: 0.0138 - val_mae_real: 0.3474 - val_rmse_real: 0.5192\n",
      "Epoch 127/200\n",
      "194/194 - 1s - loss: 0.0121 - mae_real: 0.3234 - rmse_real: 0.4801 - val_loss: 0.0138 - val_mae_real: 0.3473 - val_rmse_real: 0.5193\n",
      "\n",
      "Epoch 00127: ReduceLROnPlateau reducing learning rate to 1.9531250927684596e-06.\n",
      "Epoch 128/200\n",
      "194/194 - 1s - loss: 0.0121 - mae_real: 0.3220 - rmse_real: 0.4768 - val_loss: 0.0138 - val_mae_real: 0.3471 - val_rmse_real: 0.5195\n",
      "Epoch 129/200\n",
      "194/194 - 1s - loss: 0.0121 - mae_real: 0.3231 - rmse_real: 0.4787 - val_loss: 0.0138 - val_mae_real: 0.3472 - val_rmse_real: 0.5195\n",
      "Epoch 130/200\n",
      "194/194 - 1s - loss: 0.0121 - mae_real: 0.3224 - rmse_real: 0.4776 - val_loss: 0.0138 - val_mae_real: 0.3472 - val_rmse_real: 0.5195\n",
      "Epoch 131/200\n",
      "194/194 - 1s - loss: 0.0121 - mae_real: 0.3232 - rmse_real: 0.4790 - val_loss: 0.0138 - val_mae_real: 0.3472 - val_rmse_real: 0.5194\n",
      "Epoch 132/200\n",
      "194/194 - 1s - loss: 0.0121 - mae_real: 0.3242 - rmse_real: 0.4809 - val_loss: 0.0138 - val_mae_real: 0.3472 - val_rmse_real: 0.5194\n",
      "Epoch 133/200\n",
      "194/194 - 1s - loss: 0.0121 - mae_real: 0.3227 - rmse_real: 0.4772 - val_loss: 0.0138 - val_mae_real: 0.3471 - val_rmse_real: 0.5195\n",
      "\n",
      "Epoch 00133: ReduceLROnPlateau reducing learning rate to 1e-06.\n",
      "Epoch 134/200\n",
      "194/194 - 1s - loss: 0.0121 - mae_real: 0.3230 - rmse_real: 0.4785 - val_loss: 0.0138 - val_mae_real: 0.3471 - val_rmse_real: 0.5194\n",
      "Epoch 135/200\n",
      "194/194 - 1s - loss: 0.0121 - mae_real: 0.3237 - rmse_real: 0.4787 - val_loss: 0.0138 - val_mae_real: 0.3471 - val_rmse_real: 0.5195\n",
      "Epoch 136/200\n",
      "194/194 - 1s - loss: 0.0121 - mae_real: 0.3227 - rmse_real: 0.4786 - val_loss: 0.0138 - val_mae_real: 0.3471 - val_rmse_real: 0.5195\n",
      "Epoch 137/200\n",
      "194/194 - 1s - loss: 0.0121 - mae_real: 0.3219 - rmse_real: 0.4773 - val_loss: 0.0138 - val_mae_real: 0.3471 - val_rmse_real: 0.5195\n",
      "Epoch 138/200\n",
      "194/194 - 1s - loss: 0.0121 - mae_real: 0.3225 - rmse_real: 0.4797 - val_loss: 0.0138 - val_mae_real: 0.3471 - val_rmse_real: 0.5194\n",
      "Epoch 139/200\n",
      "194/194 - 1s - loss: 0.0121 - mae_real: 0.3221 - rmse_real: 0.4772 - val_loss: 0.0138 - val_mae_real: 0.3471 - val_rmse_real: 0.5194\n",
      "Epoch 140/200\n",
      "194/194 - 1s - loss: 0.0121 - mae_real: 0.3228 - rmse_real: 0.4779 - val_loss: 0.0138 - val_mae_real: 0.3471 - val_rmse_real: 0.5194\n",
      "Epoch 141/200\n",
      "194/194 - 1s - loss: 0.0121 - mae_real: 0.3228 - rmse_real: 0.4784 - val_loss: 0.0138 - val_mae_real: 0.3470 - val_rmse_real: 0.5195\n",
      "Epoch 142/200\n",
      "194/194 - 1s - loss: 0.0121 - mae_real: 0.3227 - rmse_real: 0.4793 - val_loss: 0.0138 - val_mae_real: 0.3471 - val_rmse_real: 0.5194\n",
      "Epoch 143/200\n",
      "194/194 - 1s - loss: 0.0121 - mae_real: 0.3228 - rmse_real: 0.4794 - val_loss: 0.0138 - val_mae_real: 0.3471 - val_rmse_real: 0.5194\n",
      "Epoch 144/200\n",
      "194/194 - 1s - loss: 0.0120 - mae_real: 0.3218 - rmse_real: 0.4772 - val_loss: 0.0138 - val_mae_real: 0.3471 - val_rmse_real: 0.5194\n",
      "Epoch 145/200\n",
      "194/194 - 1s - loss: 0.0121 - mae_real: 0.3229 - rmse_real: 0.4778 - val_loss: 0.0138 - val_mae_real: 0.3471 - val_rmse_real: 0.5193\n",
      "Epoch 146/200\n",
      "194/194 - 1s - loss: 0.0121 - mae_real: 0.3224 - rmse_real: 0.4783 - val_loss: 0.0138 - val_mae_real: 0.3471 - val_rmse_real: 0.5194\n",
      "Epoch 147/200\n",
      "194/194 - 1s - loss: 0.0121 - mae_real: 0.3235 - rmse_real: 0.4790 - val_loss: 0.0138 - val_mae_real: 0.3471 - val_rmse_real: 0.5194\n",
      "Epoch 148/200\n",
      "194/194 - 1s - loss: 0.0121 - mae_real: 0.3231 - rmse_real: 0.4804 - val_loss: 0.0138 - val_mae_real: 0.3471 - val_rmse_real: 0.5194\n",
      "Epoch 149/200\n",
      "194/194 - 1s - loss: 0.0121 - mae_real: 0.3224 - rmse_real: 0.4784 - val_loss: 0.0138 - val_mae_real: 0.3471 - val_rmse_real: 0.5194\n",
      "Epoch 150/200\n",
      "194/194 - 1s - loss: 0.0121 - mae_real: 0.3224 - rmse_real: 0.4780 - val_loss: 0.0138 - val_mae_real: 0.3470 - val_rmse_real: 0.5195\n",
      "Epoch 151/200\n",
      "194/194 - 1s - loss: 0.0120 - mae_real: 0.3223 - rmse_real: 0.4787 - val_loss: 0.0138 - val_mae_real: 0.3471 - val_rmse_real: 0.5194\n",
      "Epoch 152/200\n",
      "194/194 - 1s - loss: 0.0121 - mae_real: 0.3229 - rmse_real: 0.4792 - val_loss: 0.0138 - val_mae_real: 0.3471 - val_rmse_real: 0.5194\n",
      "Epoch 153/200\n",
      "194/194 - 1s - loss: 0.0121 - mae_real: 0.3235 - rmse_real: 0.4783 - val_loss: 0.0138 - val_mae_real: 0.3471 - val_rmse_real: 0.5194\n",
      "Epoch 154/200\n",
      "194/194 - 1s - loss: 0.0121 - mae_real: 0.3220 - rmse_real: 0.4780 - val_loss: 0.0138 - val_mae_real: 0.3471 - val_rmse_real: 0.5195\n",
      "Epoch 155/200\n",
      "194/194 - 1s - loss: 0.0121 - mae_real: 0.3230 - rmse_real: 0.4795 - val_loss: 0.0138 - val_mae_real: 0.3471 - val_rmse_real: 0.5195\n",
      "Epoch 156/200\n",
      "194/194 - 1s - loss: 0.0121 - mae_real: 0.3235 - rmse_real: 0.4790 - val_loss: 0.0138 - val_mae_real: 0.3471 - val_rmse_real: 0.5195\n",
      "Epoch 157/200\n",
      "194/194 - 1s - loss: 0.0121 - mae_real: 0.3226 - rmse_real: 0.4781 - val_loss: 0.0138 - val_mae_real: 0.3471 - val_rmse_real: 0.5195\n",
      "Epoch 158/200\n",
      "194/194 - 1s - loss: 0.0121 - mae_real: 0.3233 - rmse_real: 0.4806 - val_loss: 0.0138 - val_mae_real: 0.3471 - val_rmse_real: 0.5195\n",
      "Epoch 159/200\n",
      "194/194 - 1s - loss: 0.0121 - mae_real: 0.3221 - rmse_real: 0.4768 - val_loss: 0.0138 - val_mae_real: 0.3471 - val_rmse_real: 0.5195\n",
      "Epoch 160/200\n",
      "194/194 - 1s - loss: 0.0121 - mae_real: 0.3225 - rmse_real: 0.4784 - val_loss: 0.0138 - val_mae_real: 0.3471 - val_rmse_real: 0.5194\n",
      "Epoch 161/200\n",
      "194/194 - 1s - loss: 0.0120 - mae_real: 0.3215 - rmse_real: 0.4760 - val_loss: 0.0138 - val_mae_real: 0.3471 - val_rmse_real: 0.5194\n",
      "Epoch 162/200\n",
      "194/194 - 1s - loss: 0.0120 - mae_real: 0.3226 - rmse_real: 0.4784 - val_loss: 0.0138 - val_mae_real: 0.3471 - val_rmse_real: 0.5195\n",
      "Epoch 163/200\n",
      "194/194 - 1s - loss: 0.0121 - mae_real: 0.3231 - rmse_real: 0.4793 - val_loss: 0.0138 - val_mae_real: 0.3471 - val_rmse_real: 0.5194\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test (original scale):\n",
      "65/65 [==============================] - 0s 2ms/step - loss: 0.0130 - mae_real: 0.3366 - rmse_real: 0.5038\n",
      "loss: 0.0130\n",
      "mae_real: 0.3366\n",
      "rmse_real: 0.5038\n",
      "\n",
      "Sample predictions vs actual (units ~$100k):\n",
      "00  Pred: 0.775  Actual: 0.413\n",
      "01  Pred: 0.689  Actual: 0.690\n",
      "02  Pred: 2.100  Actual: 1.675\n",
      "03  Pred: 1.780  Actual: 2.359\n",
      "04  Pred: 0.867  Actual: 0.584\n"
     ]
    }
   ],
   "source": [
    "import os, random, numpy as np, tensorflow as tf\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Reproducibility\n",
    "SEED=42\n",
    "os.environ[\"PYTHONHASHSEED\"]=str(SEED)\n",
    "random.seed(SEED); np.random.seed(SEED); tf.random.set_seed(SEED)\n",
    "\n",
    "# ---------- Data ----------\n",
    "data = fetch_california_housing()  # returns NumPy arrays on older sklearn\n",
    "X = data.data.astype(\"float32\")\n",
    "y = data.target.astype(\"float32\")  # in $100k\n",
    "\n",
    "# Log-transform target\n",
    "y_log = np.log1p(y)\n",
    "\n",
    "X_train, X_tmp, y_train, y_tmp = train_test_split(X, y_log, test_size=0.4, random_state=SEED)\n",
    "X_val,   X_test, y_val,  y_test = train_test_split(X_tmp, y_tmp,  test_size=0.5, random_state=SEED)\n",
    "\n",
    "BATCH=64\n",
    "def ds(x,y,train=False):\n",
    "    d=tf.data.Dataset.from_tensor_slices((x,y))\n",
    "    if train: d=d.shuffle(len(x), seed=SEED)\n",
    "    return d.batch(BATCH).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "train_ds=ds(X_train,y_train,True); val_ds=ds(X_val,y_val); test_ds=ds(X_test,y_test)\n",
    "\n",
    "norm = tf.keras.layers.Normalization(axis=-1)\n",
    "norm.adapt(X_train)\n",
    "\n",
    "def mae_real(y_true_log, y_pred_log):\n",
    "    y_true = tf.math.expm1(y_true_log)\n",
    "    y_pred = tf.math.expm1(y_pred_log)\n",
    "    return tf.reduce_mean(tf.abs(y_true - y_pred))\n",
    "\n",
    "def rmse_real(y_true_log, y_pred_log):\n",
    "    y_true = tf.math.expm1(y_true_log)\n",
    "    y_pred = tf.math.expm1(y_pred_log)\n",
    "    return tf.sqrt(tf.reduce_mean(tf.square(y_true - y_pred)))\n",
    "\n",
    "# ---------- Residual MLP block ----------\n",
    "from tensorflow.keras import layers, regularizers\n",
    "\n",
    "def res_block(x, units, l2=1e-4, drop=0.1):\n",
    "    skip = x\n",
    "    x = layers.Dense(units, activation=\"relu\",\n",
    "                     kernel_initializer=\"he_normal\",\n",
    "                     kernel_regularizer=regularizers.l2(l2))(x)\n",
    "    x = layers.Dropout(drop)(x)\n",
    "    x = layers.Dense(units, activation=None,\n",
    "                     kernel_initializer=\"he_normal\",\n",
    "                     kernel_regularizer=regularizers.l2(l2))(x)\n",
    "    # match shapes for residual (if needed)\n",
    "    if skip.shape[-1] != units:\n",
    "        skip = layers.Dense(units, activation=None,\n",
    "                            kernel_regularizer=regularizers.l2(l2))(skip)\n",
    "    x = layers.Add()([x, skip])\n",
    "    x = layers.Activation(\"relu\")(x)\n",
    "    return x\n",
    "\n",
    "# ---------- Model ----------\n",
    "inputs = tf.keras.Input(shape=(X.shape[1],))\n",
    "x = norm(inputs)\n",
    "\n",
    "# Moderately wide with residuals (often better than many small layers)\n",
    "for u in [128, 128, 64]:\n",
    "    x = res_block(x, u, l2=1e-4, drop=0.1)\n",
    "\n",
    "outputs = layers.Dense(1, activation=\"linear\")(x)  # predicts log-price\n",
    "model = tf.keras.Model(inputs, outputs)\n",
    "\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(1e-3),\n",
    "    loss=tf.keras.losses.Huber(delta=1.0),  # in log-space\n",
    "    metrics=[mae_real, rmse_real]\n",
    ")\n",
    "\n",
    "callbacks = [\n",
    "    tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=12, restore_best_weights=True),\n",
    "    tf.keras.callbacks.ReduceLROnPlateau(monitor=\"val_loss\", factor=0.5, patience=6, min_lr=1e-6, verbose=1)\n",
    "]\n",
    "\n",
    "history = model.fit(train_ds, validation_data=val_ds, epochs=200, callbacks=callbacks, verbose=2)\n",
    "\n",
    "print(\"\\nTest (original scale):\")\n",
    "test = model.evaluate(test_ds, return_dict=True)\n",
    "for k,v in test.items(): print(f\"{k}: {v:.4f}\")\n",
    "\n",
    "# Sample preds back in original scale\n",
    "pred_log = model.predict(X_test[:5]).flatten()\n",
    "pred = np.expm1(pred_log)\n",
    "actual = np.expm1(y_test[:5])\n",
    "print(\"\\nSample predictions vs actual (units ~$100k):\")\n",
    "for i,(p,a) in enumerate(zip(pred, actual)):\n",
    "    print(f\"{i:02d}  Pred: {p:.3f}  Actual: {a:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://chatgpt.com/share/68ce3c99-afa4-8010-951a-9304e87495f3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow2",
   "language": "python",
   "name": "tensorflow2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
